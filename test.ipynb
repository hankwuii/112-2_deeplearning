{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import Module, Sequential\n",
    "from torch.nn import Linear, ReLU, LeakyReLU\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "file_path_train=\"C:\\Users\\david\\Desktop\\test\\Google_Stock_Price\\Google_Stock_Price_Train.csv\"\n",
    "file_path_test=\"C:\\Users\\david\\Desktop\\test\\Google_Stock_Price\\Google_Stock_Price_Test.csv\"\n",
    "\n",
    "df_train = pd.read_csv(file_path_train)\n",
    "df_test = pd.read_csv(file_path_test)\n",
    "\n",
    "df_train['Date'] = pd.to_datetime(df_train['Date'], format='%m/%d/%Y')\n",
    "df_test['Date'] = pd.to_datetime(df_test['Date'], format='%m/%d/%Y')\n",
    "df_train['Volume'] = df_train['Volume'].str.replace(',', '').astype(int)\n",
    "df_test['Volume'] = df_test['Volume'].str.replace(',', '').astype(int)\n",
    "\n",
    "X_train = df_train[['Open', 'High', 'Low', 'Close', 'Volume']].values\n",
    "X_test = df_test[['Open', 'High', 'Low', 'Close', 'Volume']].values\n",
    "y_train = df_train['Close'].values.reshape(-1, 1)  # Assuming you want to predict the 'Close' price\n",
    "y_test = df_test['Close'].values.reshape(-1, 1)  # Assuming you want to predict the 'Close' price\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "y_train_scaled = scaler.fit_transform(y_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "y_test_scaled = scaler.fit_transform(y_test)\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_scaled, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network architecture\n",
    "class Neural_Network(nn.Module):\n",
    "    def __init__(self, num_features: int, num_classes: int):\n",
    "        super(Neural_Network, self).__init__()\n",
    "\n",
    "        self.layer_1_neurons = 40\n",
    "        self.layer_2_neurons = 20\n",
    "        self.layer_3_neurons = 10\n",
    "        self.output_neurons = 6  # Number of neurons in the last hidden layer\n",
    "\n",
    "        self.fc_input = nn.Sequential(\n",
    "            nn.Linear(num_features, self.layer_1_neurons),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.fc_l1 = nn.Sequential(\n",
    "            nn.Linear(self.layer_1_neurons, self.layer_2_neurons),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.fc_l2 = nn.Sequential(\n",
    "            nn.Linear(self.layer_2_neurons, self.layer_3_neurons),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.fc_output = nn.Sequential(\n",
    "            nn.Linear(self.layer_3_neurons, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Linear regression layer\n",
    "        self.linear_regression = nn.Linear(self.output_neurons + num_features, 1)\n",
    "\n",
    "        self.optimizer = Adam(self.parameters(), lr=0.00005)\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_input(x)\n",
    "        x = self.fc_l1(x)\n",
    "        x = self.fc_l2(x)\n",
    "        output_nn = self.fc_output(x)\n",
    "        output_concat = torch.cat((output_nn, x), dim=1)  # Concatenate output with original features\n",
    "        output_linear = self.linear_regression(output_concat)\n",
    "        return output_linear\n",
    "\n",
    "    def train_model(self, train_loader, val_loader, num_epochs=300):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            epoch_train_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.forward(inputs)\n",
    "                loss = self.loss_fn(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_train_loss += loss.item() * inputs.size(0)\n",
    "            epoch_train_loss /= len(train_loader.dataset)\n",
    "            train_losses.append(epoch_train_loss)\n",
    "\n",
    "            # Validation loss\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                epoch_val_loss = 0.0\n",
    "                for inputs, labels in val_loader:\n",
    "                    outputs = self.forward(inputs)\n",
    "                    loss = self.loss_fn(outputs, labels)\n",
    "                    epoch_val_loss += loss.item() * inputs.size(0)\n",
    "                epoch_val_loss /= len(val_loader.dataset)\n",
    "                val_losses.append(epoch_val_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss}, Validation Loss: {epoch_val_loss}\")\n",
    "\n",
    "        # Plot loss curves\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = self.forward(X_test)\n",
    "            mae = mean_absolute_error(y_test.detach().numpy(), predictions.detach().numpy())\n",
    "            mse = mean_squared_error(y_test.detach().numpy(), predictions.detach().numpy())\n",
    "            print(f\"MAE: {mae}, MSE: {mse}\")\n",
    "            return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for both training and validation\n",
    "X_train_tensor, X_val_tensor, y_train_tensor, y_val_tensor = train_test_split(X_train_tensor, y_train_tensor, test_size=0.2)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Create an instance of the neural network model\n",
    "num_features = X_train_tensor.shape[1]\n",
    "num_classes = 1  # Regression has one output\n",
    "model = Neural_Network(num_features, num_classes)\n",
    "\n",
    "# Train the model\n",
    "model.train_model(train_loader, val_loader)\n",
    "\n",
    "# Evaluate the model on the test set and print performance metrics\n",
    "predictions = model.evaluate_model(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Denormalize predictions\n",
    "predictions_denormalized = scaler.inverse_transform(predictions.detach().numpy())\n",
    "y_test_denormalized = scaler.inverse_transform(y_test_tensor.numpy())\n",
    "\n",
    "# Plot actual vs. predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting the actual test data\n",
    "plt.plot(y_test_denormalized, color='blue', label='Actual')\n",
    "\n",
    "# Plotting the predicted data\n",
    "plt.plot(predictions_denormalized, color='red', label='Predicted')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Close Price')\n",
    "plt.title('Actual vs. Predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import Module, Sequential\n",
    "from torch.nn import Linear, ReLU, LeakyReLU\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Load and preprocess data\n",
    "file_path_train=r\"C:\\Users\\david\\Desktop\\test\\Google_Stock_Price\\Google_Stock_Price_Train.csv\"\n",
    "file_path_test=r\"C:\\Users\\david\\Desktop\\test\\Google_Stock_Price\\Google_Stock_Price_Test.csv\"\n",
    "\n",
    "df_train = pd.read_csv(file_path_train)\n",
    "df_test = pd.read_csv(file_path_test)\n",
    "\n",
    "df_train['Date'] = pd.to_datetime(df_train['Date'], format='%m/%d/%Y')\n",
    "df_test['Date'] = pd.to_datetime(df_test['Date'], format='%m/%d/%Y')\n",
    "df_train['Volume'] = df_train['Volume'].str.replace(',', '').astype(int)\n",
    "df_test['Volume'] = df_test['Volume'].str.replace(',', '').astype(int)\n",
    "\n",
    "X_train = df_train[['Open', 'High', 'Low', 'Close', 'Volume']].values\n",
    "X_test = df_test[['Open', 'High', 'Low', 'Close', 'Volume']].values\n",
    "y_train = df_train['Close'].values.reshape(-1, 1)  # Assuming you want to predict the 'Close' price\n",
    "y_test = df_test['Close'].values.reshape(-1, 1)  # Assuming you want to predict the 'Close' price\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "y_train_scaled = scaler.fit_transform(y_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "y_test_scaled = scaler.fit_transform(y_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_scaled, dtype=torch.float32)\n",
    "\n",
    "# Define neural network architecture\n",
    "class Neural_Network(nn.Module):\n",
    "    def __init__(self, num_features: int, num_classes: int):\n",
    "        super(Neural_Network, self).__init__()\n",
    "\n",
    "        self.layer_1_neurons = 40\n",
    "        self.layer_2_neurons = 20\n",
    "        self.layer_3_neurons = 10\n",
    "        self.output_neurons = 6  # Number of neurons in the last hidden layer\n",
    "\n",
    "        self.fc_input = nn.Sequential(\n",
    "            nn.Linear(num_features, self.layer_1_neurons),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.fc_l1 = nn.Sequential(\n",
    "            nn.Linear(self.layer_1_neurons, self.layer_2_neurons),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.fc_l2 = nn.Sequential(\n",
    "            nn.Linear(self.layer_2_neurons, self.layer_3_neurons),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.fc_output = nn.Sequential(\n",
    "            nn.Linear(self.layer_3_neurons, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Linear regression layer\n",
    "        self.linear_regression = nn.Linear(self.output_neurons + num_features, 1)\n",
    "\n",
    "        self.optimizer = Adam(self.parameters(), lr=0.00005)\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_input(x)\n",
    "        x = self.fc_l1(x)\n",
    "        x = self.fc_l2(x)\n",
    "        output_nn = self.fc_output(x)\n",
    "        output_concat = torch.cat((output_nn, x), dim=1)  # Concatenate output with original features\n",
    "        output_linear = self.linear_regression(output_concat)\n",
    "        return output_linear\n",
    "\n",
    "    def train_model(self, train_loader, val_loader, num_epochs=300):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            epoch_train_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.forward(inputs)\n",
    "                loss = self.loss_fn(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_train_loss += loss.item() * inputs.size(0)\n",
    "            epoch_train_loss /= len(train_loader.dataset)\n",
    "            train_losses.append(epoch_train_loss)\n",
    "\n",
    "            # Validation loss\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                epoch_val_loss = 0.0\n",
    "                for inputs, labels in val_loader:\n",
    "                    outputs = self.forward(inputs)\n",
    "                    loss = self.loss_fn(outputs, labels)\n",
    "                    epoch_val_loss += loss.item() * inputs.size(0)\n",
    "                epoch_val_loss /= len(val_loader.dataset)\n",
    "                val_losses.append(epoch_val_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss}, Validation Loss: {epoch_val_loss}\")\n",
    "\n",
    "        # Plot loss curves\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = self.forward(X_test)\n",
    "            mae = mean_absolute_error(y_test.detach().numpy(), predictions.detach().numpy())\n",
    "            mse = mean_squared_error(y_test.detach().numpy(), predictions.detach().numpy())\n",
    "            print(f\"MAE: {mae}, MSE: {mse}\")\n",
    "            return predictions\n",
    "        \n",
    "# Create DataLoader for both training and validation\n",
    "X_train_tensor, X_val_tensor, y_train_tensor, y_val_tensor = train_test_split(X_train_tensor, y_train_tensor, test_size=0.2)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# Create an instance of the neural network model\n",
    "num_features = X_train_tensor.shape[1]\n",
    "num_classes = 1  # Regression has one output\n",
    "model = Neural_Network(num_features, num_classes)\n",
    "\n",
    "# Train the model\n",
    "model.train_model(train_loader, val_loader)\n",
    "\n",
    "# Evaluate the model on the test set and print performance metrics\n",
    "predictions = model.evaluate_model(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Denormalize predictions\n",
    "predictions_denormalized = scaler.inverse_transform(predictions.detach().numpy())\n",
    "y_test_denormalized = scaler.inverse_transform(y_test_tensor.numpy())\n",
    "\n",
    "# Plot actual vs. predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plotting the actual test data\n",
    "plt.plot(y_test_denormalized, color='blue', label='Actual')\n",
    "\n",
    "# Plotting the predicted data\n",
    "plt.plot(predictions_denormalized, color='red', label='Predicted')\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Close Price')\n",
    "plt.title('Actual vs. Predicted')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Train Loss: 1.042684021811362, Validation Loss: 0.945002017513154\n",
      "\n",
      "Epoch 2/200, Train Loss: 1.0347372981947411, Validation Loss: 0.9381377971361554\n",
      "\n",
      "Epoch 3/200, Train Loss: 1.0267731354203424, Validation Loss: 0.931323544373588\n",
      "\n",
      "Epoch 4/200, Train Loss: 1.018656715484073, Validation Loss: 0.924555084062001\n",
      "\n",
      "Epoch 5/200, Train Loss: 1.0106825532306496, Validation Loss: 0.9175454056452191\n",
      "\n",
      "Epoch 6/200, Train Loss: 1.0023132107603858, Validation Loss: 0.9107070196242559\n",
      "\n",
      "Epoch 7/200, Train Loss: 0.9940234662050281, Validation Loss: 0.9028186154743981\n",
      "\n",
      "Epoch 8/200, Train Loss: 0.9849161258037711, Validation Loss: 0.8947745485911294\n",
      "\n",
      "Epoch 9/200, Train Loss: 0.976214546925031, Validation Loss: 0.8868563156279307\n",
      "\n",
      "Epoch 10/200, Train Loss: 0.9673480179390423, Validation Loss: 0.878839960173955\n",
      "\n",
      "Epoch 11/200, Train Loss: 0.9583227859576701, Validation Loss: 0.8701461846866305\n",
      "\n",
      "Epoch 12/200, Train Loss: 0.9486934906445725, Validation Loss: 0.8611771804945809\n",
      "\n",
      "Epoch 13/200, Train Loss: 0.9386774979339205, Validation Loss: 0.8514149113306924\n",
      "\n",
      "Epoch 14/200, Train Loss: 0.928198089419492, Validation Loss: 0.8416214310933673\n",
      "\n",
      "Epoch 15/200, Train Loss: 0.9180003997108809, Validation Loss: 0.8320250161110408\n",
      "\n",
      "Epoch 16/200, Train Loss: 0.9079961871532038, Validation Loss: 0.8223351183391753\n",
      "\n",
      "Epoch 17/200, Train Loss: 0.8976750202017797, Validation Loss: 0.8135415854908171\n",
      "\n",
      "Epoch 18/200, Train Loss: 0.8879255767371024, Validation Loss: 0.8039911777254135\n",
      "\n",
      "Epoch 19/200, Train Loss: 0.877799223715932, Validation Loss: 0.7945949113558209\n",
      "\n",
      "Epoch 20/200, Train Loss: 0.8675047637928077, Validation Loss: 0.7852350454481821\n",
      "\n",
      "Epoch 21/200, Train Loss: 0.8575141646753012, Validation Loss: 0.7755026107742673\n",
      "\n",
      "Epoch 22/200, Train Loss: 0.8468984526620944, Validation Loss: 0.7660245242572966\n",
      "\n",
      "Epoch 23/200, Train Loss: 0.8365179677132821, Validation Loss: 0.755889514135936\n",
      "\n",
      "Epoch 24/200, Train Loss: 0.8256893271717353, Validation Loss: 0.745732307434082\n",
      "\n",
      "Epoch 25/200, Train Loss: 0.8146902745808096, Validation Loss: 0.7353607018788656\n",
      "\n",
      "Epoch 26/200, Train Loss: 0.8032291828993537, Validation Loss: 0.7251362403233846\n",
      "\n",
      "Epoch 27/200, Train Loss: 0.7918203628323898, Validation Loss: 0.7144256659916469\n",
      "\n",
      "Epoch 28/200, Train Loss: 0.7800644602974651, Validation Loss: 0.7035024885147337\n",
      "\n",
      "Epoch 29/200, Train Loss: 0.7681715755055012, Validation Loss: 0.6921897861692641\n",
      "\n",
      "Epoch 30/200, Train Loss: 0.7558549727881642, Validation Loss: 0.6807922214742691\n",
      "\n",
      "Epoch 31/200, Train Loss: 0.7430960645021548, Validation Loss: 0.668496639009506\n",
      "\n",
      "Epoch 32/200, Train Loss: 0.7294889213313639, Validation Loss: 0.6550979973777892\n",
      "\n",
      "Epoch 33/200, Train Loss: 0.714035928604854, Validation Loss: 0.6396016275125836\n",
      "\n",
      "Epoch 34/200, Train Loss: 0.6970877995192412, Validation Loss: 0.6224871694095551\n",
      "\n",
      "Epoch 35/200, Train Loss: 0.6793615076224325, Validation Loss: 0.6052579804072304\n",
      "\n",
      "Epoch 36/200, Train Loss: 0.6607028812347776, Validation Loss: 0.5889735557730236\n",
      "\n",
      "Epoch 37/200, Train Loss: 0.6425643854776386, Validation Loss: 0.5728722101166135\n",
      "\n",
      "Epoch 38/200, Train Loss: 0.6248568176749212, Validation Loss: 0.5562848902883983\n",
      "\n",
      "Epoch 39/200, Train Loss: 0.6064490798571949, Validation Loss: 0.5405252689406985\n",
      "\n",
      "Epoch 40/200, Train Loss: 0.5889420063784772, Validation Loss: 0.5240954718892537\n",
      "\n",
      "Epoch 41/200, Train Loss: 0.5705710970620749, Validation Loss: 0.5083303219742246\n",
      "\n",
      "Epoch 42/200, Train Loss: 0.5526610986138907, Validation Loss: 0.4916741663501376\n",
      "\n",
      "Epoch 43/200, Train Loss: 0.534525729552889, Validation Loss: 0.47510530172832427\n",
      "\n",
      "Epoch 44/200, Train Loss: 0.5167102992653135, Validation Loss: 0.4583951797750261\n",
      "\n",
      "Epoch 45/200, Train Loss: 0.49849969344158057, Validation Loss: 0.44230342289758107\n",
      "\n",
      "Epoch 46/200, Train Loss: 0.48063883748253583, Validation Loss: 0.4268958100250789\n",
      "\n",
      "Epoch 47/200, Train Loss: 0.4638965506676888, Validation Loss: 0.41090868200574604\n",
      "\n",
      "Epoch 48/200, Train Loss: 0.4463424856217197, Validation Loss: 0.395921632410988\n",
      "\n",
      "Epoch 49/200, Train Loss: 0.4298827023676802, Validation Loss: 0.3810155571453155\n",
      "\n",
      "Epoch 50/200, Train Loss: 0.4134749525465501, Validation Loss: 0.3666834504831405\n",
      "\n",
      "Epoch 51/200, Train Loss: 0.39772688098266634, Validation Loss: 0.3523499625069754\n",
      "\n",
      "Epoch 52/200, Train Loss: 0.3821639950066864, Validation Loss: 0.3381089111642232\n",
      "\n",
      "Epoch 53/200, Train Loss: 0.3666730545624113, Validation Loss: 0.3244359015472352\n",
      "\n",
      "Epoch 54/200, Train Loss: 0.351528003130471, Validation Loss: 0.31101589567131466\n",
      "\n",
      "Epoch 55/200, Train Loss: 0.33655800988849544, Validation Loss: 0.29737034085251035\n",
      "\n",
      "Epoch 56/200, Train Loss: 0.3213617809368649, Validation Loss: 0.2835584200090832\n",
      "\n",
      "Epoch 57/200, Train Loss: 0.30617690521845997, Validation Loss: 0.26993980317834826\n",
      "\n",
      "Epoch 58/200, Train Loss: 0.2910288620303213, Validation Loss: 0.256732451064246\n",
      "\n",
      "Epoch 59/200, Train Loss: 0.2764267120991739, Validation Loss: 0.2435985706628315\n",
      "\n",
      "Epoch 60/200, Train Loss: 0.2618757085226639, Validation Loss: 0.23087446580803583\n",
      "\n",
      "Epoch 61/200, Train Loss: 0.2477672252337454, Validation Loss: 0.2185301161001599\n",
      "\n",
      "Epoch 62/200, Train Loss: 0.23418757639395787, Validation Loss: 0.20647440519597796\n",
      "\n",
      "Epoch 63/200, Train Loss: 0.22080695679481652, Validation Loss: 0.19509455916427432\n",
      "\n",
      "Epoch 64/200, Train Loss: 0.208257045441309, Validation Loss: 0.1838873482885815\n",
      "\n",
      "Epoch 65/200, Train Loss: 0.19614876421616517, Validation Loss: 0.17337091077887823\n",
      "\n",
      "Epoch 66/200, Train Loss: 0.18461675068019162, Validation Loss: 0.1635612915196116\n",
      "\n",
      "Epoch 67/200, Train Loss: 0.17387488715454313, Validation Loss: 0.1541006259974979\n",
      "\n",
      "Epoch 68/200, Train Loss: 0.16349807544565106, Validation Loss: 0.14527542676244462\n",
      "\n",
      "Epoch 69/200, Train Loss: 0.15391836387973656, Validation Loss: 0.13673524582196797\n",
      "\n",
      "Epoch 70/200, Train Loss: 0.1446969310343858, Validation Loss: 0.1286739113311919\n",
      "\n",
      "Epoch 71/200, Train Loss: 0.13581789778193706, Validation Loss: 0.12126949961696352\n",
      "\n",
      "Epoch 72/200, Train Loss: 0.12774393870448972, Validation Loss: 0.11412043763058526\n",
      "\n",
      "Epoch 73/200, Train Loss: 0.11991824454092838, Validation Loss: 0.10739411330885357\n",
      "\n",
      "Epoch 74/200, Train Loss: 0.11277888189430976, Validation Loss: 0.1011497297930339\n",
      "\n",
      "Epoch 75/200, Train Loss: 0.1060596373872776, Validation Loss: 0.095357089288651\n",
      "\n",
      "Epoch 76/200, Train Loss: 0.09972994331692607, Validation Loss: 0.09000009191887719\n",
      "\n",
      "Epoch 77/200, Train Loss: 0.09396315272712329, Validation Loss: 0.08481904225690025\n",
      "\n",
      "Epoch 78/200, Train Loss: 0.0883547275606728, Validation Loss: 0.07990772365814164\n",
      "\n",
      "Epoch 79/200, Train Loss: 0.08308206251257694, Validation Loss: 0.07528510234422153\n",
      "\n",
      "Epoch 80/200, Train Loss: 0.07811453581508064, Validation Loss: 0.0708934652308623\n",
      "\n",
      "Epoch 81/200, Train Loss: 0.07347439232450831, Validation Loss: 0.06678896421004855\n",
      "\n",
      "Epoch 82/200, Train Loss: 0.06915019483677673, Validation Loss: 0.06294535342899579\n",
      "\n",
      "Epoch 83/200, Train Loss: 0.06507597708145145, Validation Loss: 0.05932879447937012\n",
      "\n",
      "Epoch 84/200, Train Loss: 0.06123714169735463, Validation Loss: 0.055926231756096796\n",
      "\n",
      "Epoch 85/200, Train Loss: 0.05761775197405701, Validation Loss: 0.05271572761592411\n",
      "\n",
      "Epoch 86/200, Train Loss: 0.054213062439773, Validation Loss: 0.04960456354514001\n",
      "\n",
      "Epoch 87/200, Train Loss: 0.05096769610616132, Validation Loss: 0.046684395699273976\n",
      "\n",
      "Epoch 88/200, Train Loss: 0.04783684358919117, Validation Loss: 0.043918874114751816\n",
      "\n",
      "Epoch 89/200, Train Loss: 0.04496310348391059, Validation Loss: 0.04122270735365058\n",
      "\n",
      "Epoch 90/200, Train Loss: 0.0421945988746808, Validation Loss: 0.03865111084093177\n",
      "\n",
      "Epoch 91/200, Train Loss: 0.03952612818976046, Validation Loss: 0.03627373118485723\n",
      "\n",
      "Epoch 92/200, Train Loss: 0.03706897593753238, Validation Loss: 0.03396594695865162\n",
      "\n",
      "Epoch 93/200, Train Loss: 0.03469056138402188, Validation Loss: 0.031846063270691845\n",
      "\n",
      "Epoch 94/200, Train Loss: 0.03247515289168472, Validation Loss: 0.02983456811616345\n",
      "\n",
      "Epoch 95/200, Train Loss: 0.030411338874288867, Validation Loss: 0.027849976623815203\n",
      "\n",
      "Epoch 96/200, Train Loss: 0.02840354911257211, Validation Loss: 0.026020516656220904\n",
      "\n",
      "Epoch 97/200, Train Loss: 0.026570021339634778, Validation Loss: 0.024267479836467713\n",
      "\n",
      "Epoch 98/200, Train Loss: 0.02478178242077055, Validation Loss: 0.02267298259077564\n",
      "\n",
      "Epoch 99/200, Train Loss: 0.023194329565016936, Validation Loss: 0.02113756061428123\n",
      "\n",
      "Epoch 100/200, Train Loss: 0.021640809562064663, Validation Loss: 0.019740681001354777\n",
      "\n",
      "Epoch 101/200, Train Loss: 0.020201576847738583, Validation Loss: 0.018456028390025334\n",
      "\n",
      "Epoch 102/200, Train Loss: 0.01889791493498485, Validation Loss: 0.017184415979990885\n",
      "\n",
      "Epoch 103/200, Train Loss: 0.01765218709649196, Validation Loss: 0.01601435901922366\n",
      "\n",
      "Epoch 104/200, Train Loss: 0.016470571254078248, Validation Loss: 0.014977690246370103\n",
      "\n",
      "Epoch 105/200, Train Loss: 0.015403512498700832, Validation Loss: 0.01399524248249474\n",
      "\n",
      "Epoch 106/200, Train Loss: 0.014471807715225646, Validation Loss: 0.012998760870051763\n",
      "\n",
      "Epoch 107/200, Train Loss: 0.013490017990453936, Validation Loss: 0.012164675837589635\n",
      "\n",
      "Epoch 108/200, Train Loss: 0.012667758975392782, Validation Loss: 0.011394764533236859\n",
      "\n",
      "Epoch 109/200, Train Loss: 0.011908516948801979, Validation Loss: 0.010681724914955714\n",
      "\n",
      "Epoch 110/200, Train Loss: 0.011184263170861582, Validation Loss: 0.010028630362025328\n",
      "\n",
      "Epoch 111/200, Train Loss: 0.010551856204896988, Validation Loss: 0.009400893084054429\n",
      "\n",
      "Epoch 112/200, Train Loss: 0.009951994385918376, Validation Loss: 0.008833504592378935\n",
      "\n",
      "Epoch 113/200, Train Loss: 0.00940148993423042, Validation Loss: 0.008314614893779868\n",
      "\n",
      "Epoch 114/200, Train Loss: 0.008907312047751121, Validation Loss: 0.007833204623903073\n",
      "\n",
      "Epoch 115/200, Train Loss: 0.008437890872344464, Validation Loss: 0.007385255909332681\n",
      "\n",
      "Epoch 116/200, Train Loss: 0.008000628645004145, Validation Loss: 0.006999638766819049\n",
      "\n",
      "Epoch 117/200, Train Loss: 0.007625260427372001, Validation Loss: 0.006614454925828983\n",
      "\n",
      "Epoch 118/200, Train Loss: 0.007256957469045168, Validation Loss: 0.006307539153134539\n",
      "\n",
      "Epoch 119/200, Train Loss: 0.006939958694907531, Validation Loss: 0.0059868959443909785\n",
      "\n",
      "Epoch 120/200, Train Loss: 0.00665479568694209, Validation Loss: 0.005695247741800452\n",
      "\n",
      "Epoch 121/200, Train Loss: 0.006349352886437422, Validation Loss: 0.005442077116597267\n",
      "\n",
      "Epoch 122/200, Train Loss: 0.0061033025345067025, Validation Loss: 0.005195043759331817\n",
      "\n",
      "Epoch 123/200, Train Loss: 0.005864536068467034, Validation Loss: 0.004967313518540727\n",
      "\n",
      "Epoch 124/200, Train Loss: 0.005641152364781374, Validation Loss: 0.004768183088994452\n",
      "\n",
      "Epoch 125/200, Train Loss: 0.005441807121571679, Validation Loss: 0.00457284776198249\n",
      "\n",
      "Epoch 126/200, Train Loss: 0.005249759646143697, Validation Loss: 0.004391256013944272\n",
      "\n",
      "Epoch 127/200, Train Loss: 0.005066488126096562, Validation Loss: 0.004229634922618668\n",
      "\n",
      "Epoch 128/200, Train Loss: 0.004896181131940831, Validation Loss: 0.00407670648945939\n",
      "\n",
      "Epoch 129/200, Train Loss: 0.004742856742443876, Validation Loss: 0.003926161684036728\n",
      "\n",
      "Epoch 130/200, Train Loss: 0.00459266920012994, Validation Loss: 0.0037824520323839453\n",
      "\n",
      "Epoch 131/200, Train Loss: 0.004451148675765775, Validation Loss: 0.003648772177892545\n",
      "\n",
      "Epoch 132/200, Train Loss: 0.004319565169324369, Validation Loss: 0.003535696334900364\n",
      "\n",
      "Epoch 133/200, Train Loss: 0.004190021108421192, Validation Loss: 0.0034049348462195624\n",
      "\n",
      "Epoch 134/200, Train Loss: 0.0040627423471125, Validation Loss: 0.003306908739937676\n",
      "\n",
      "Epoch 135/200, Train Loss: 0.003953283718855641, Validation Loss: 0.0031867532229553613\n",
      "\n",
      "Epoch 136/200, Train Loss: 0.003834469185560705, Validation Loss: 0.003091365767879382\n",
      "\n",
      "Epoch 137/200, Train Loss: 0.003728970899163314, Validation Loss: 0.0029956789702058786\n",
      "\n",
      "Epoch 138/200, Train Loss: 0.003621474088486247, Validation Loss: 0.0029034143986387383\n",
      "\n",
      "Epoch 139/200, Train Loss: 0.003520620715569253, Validation Loss: 0.0028127750810531395\n",
      "\n",
      "Epoch 140/200, Train Loss: 0.0034211519577912615, Validation Loss: 0.002731817626645641\n",
      "\n",
      "Epoch 141/200, Train Loss: 0.0033282634180046575, Validation Loss: 0.00264959884733553\n",
      "\n",
      "Epoch 142/200, Train Loss: 0.003235988212919703, Validation Loss: 0.002566117841366028\n",
      "\n",
      "Epoch 143/200, Train Loss: 0.003148252952651345, Validation Loss: 0.0024784109148655144\n",
      "\n",
      "Epoch 144/200, Train Loss: 0.003057444688473378, Validation Loss: 0.0024296946875337097\n",
      "\n",
      "Epoch 145/200, Train Loss: 0.0029731561067823328, Validation Loss: 0.0023501457051477498\n",
      "\n",
      "Epoch 146/200, Train Loss: 0.0028907601544763115, Validation Loss: 0.002283732281140392\n",
      "\n",
      "Epoch 147/200, Train Loss: 0.0028083699197132533, Validation Loss: 0.0022162924176968987\n",
      "\n",
      "Epoch 148/200, Train Loss: 0.002728380605617305, Validation Loss: 0.0021533540926784987\n",
      "\n",
      "Epoch 149/200, Train Loss: 0.002653758019342219, Validation Loss: 0.00209041685235524\n",
      "\n",
      "Epoch 150/200, Train Loss: 0.002578591373235276, Validation Loss: 0.002033270176662694\n",
      "\n",
      "Epoch 151/200, Train Loss: 0.0025091590808804477, Validation Loss: 0.001974322768445644\n",
      "\n",
      "Epoch 152/200, Train Loss: 0.0024373076794109868, Validation Loss: 0.0019232201859325406\n",
      "\n",
      "Epoch 153/200, Train Loss: 0.002368164644761763, Validation Loss: 0.0018701619516673779\n",
      "\n",
      "Epoch 154/200, Train Loss: 0.002305763951367578, Validation Loss: 0.0018228455298831539\n",
      "\n",
      "Epoch 155/200, Train Loss: 0.002240220499530464, Validation Loss: 0.0017694490924773235\n",
      "\n",
      "Epoch 156/200, Train Loss: 0.0021773141070045013, Validation Loss: 0.0017218094851289476\n",
      "\n",
      "Epoch 157/200, Train Loss: 0.00211579748429358, Validation Loss: 0.0016748505101228754\n",
      "\n",
      "Epoch 158/200, Train Loss: 0.0020593919698241544, Validation Loss: 0.0016412707600032999\n",
      "\n",
      "Epoch 159/200, Train Loss: 0.0020030531231462066, Validation Loss: 0.0015897044102616963\n",
      "\n",
      "Epoch 160/200, Train Loss: 0.0019437475220224344, Validation Loss: 0.001552104486137747\n",
      "\n",
      "Epoch 161/200, Train Loss: 0.0018907139613802283, Validation Loss: 0.0015121021877146429\n",
      "\n",
      "Epoch 162/200, Train Loss: 0.0018404852362834733, Validation Loss: 0.0014697004863548848\n",
      "\n",
      "Epoch 163/200, Train Loss: 0.001789778692251586, Validation Loss: 0.001440377874551193\n",
      "\n",
      "Epoch 164/200, Train Loss: 0.0017419661539671554, Validation Loss: 0.001398337712984473\n",
      "\n",
      "Epoch 165/200, Train Loss: 0.0016945911285321175, Validation Loss: 0.0013661016229658373\n",
      "\n",
      "Epoch 166/200, Train Loss: 0.0016519566583604538, Validation Loss: 0.0013247225442827339\n",
      "\n",
      "Epoch 167/200, Train Loss: 0.0016086512320687917, Validation Loss: 0.0013175020899830593\n",
      "\n",
      "Epoch 168/200, Train Loss: 0.0015652652107323586, Validation Loss: 0.0012692776055903071\n",
      "\n",
      "Epoch 169/200, Train Loss: 0.0015245895817964737, Validation Loss: 0.0012516749538629063\n",
      "\n",
      "Epoch 170/200, Train Loss: 0.0014829608897824825, Validation Loss: 0.0012160992122343963\n",
      "\n",
      "Epoch 171/200, Train Loss: 0.0014474066777458456, Validation Loss: 0.0011839368990400718\n",
      "\n",
      "Epoch 172/200, Train Loss: 0.0014112171058304178, Validation Loss: 0.0011744169363131125\n",
      "\n",
      "Epoch 173/200, Train Loss: 0.001377404040923605, Validation Loss: 0.0011412545046325597\n",
      "\n",
      "Epoch 174/200, Train Loss: 0.0013454348030805616, Validation Loss: 0.001125781380179678\n",
      "\n",
      "Epoch 175/200, Train Loss: 0.0013162360891026757, Validation Loss: 0.0010873229632760205\n",
      "\n",
      "Epoch 176/200, Train Loss: 0.001281646752422657, Validation Loss: 0.0010759551327542535\n",
      "\n",
      "Epoch 177/200, Train Loss: 0.0012538359675437923, Validation Loss: 0.0010539025007303628\n",
      "\n",
      "Epoch 178/200, Train Loss: 0.0012254857645179358, Validation Loss: 0.0010417343813940764\n",
      "\n",
      "Epoch 179/200, Train Loss: 0.0011990175940740713, Validation Loss: 0.0010133705948228164\n",
      "\n",
      "Epoch 180/200, Train Loss: 0.0011708643261961653, Validation Loss: 0.000996080083062961\n",
      "\n",
      "Epoch 181/200, Train Loss: 0.0011459261030384578, Validation Loss: 0.0009813213458521263\n",
      "\n",
      "Epoch 182/200, Train Loss: 0.0011215797855998306, Validation Loss: 0.0009609695703219918\n",
      "\n",
      "Epoch 183/200, Train Loss: 0.001097345598218833, Validation Loss: 0.0009453774851170324\n",
      "\n",
      "Epoch 184/200, Train Loss: 0.0010760644907650222, Validation Loss: 0.0009351457387859386\n",
      "\n",
      "Epoch 185/200, Train Loss: 0.0010515222016714558, Validation Loss: 0.0009155059150154037\n",
      "\n",
      "Epoch 186/200, Train Loss: 0.0010304771570644339, Validation Loss: 0.0009100237336482793\n",
      "\n",
      "Epoch 187/200, Train Loss: 0.0010098960891941832, Validation Loss: 0.0008885999707253798\n",
      "\n",
      "Epoch 188/200, Train Loss: 0.0009910728232509218, Validation Loss: 0.0008755251589704245\n",
      "\n",
      "Epoch 189/200, Train Loss: 0.0009700503634477764, Validation Loss: 0.000869601445701269\n",
      "\n",
      "Epoch 190/200, Train Loss: 0.0009497118529180439, Validation Loss: 0.0008528247967155443\n",
      "\n",
      "Epoch 191/200, Train Loss: 0.0009323921604044958, Validation Loss: 0.0008382235278582408\n",
      "\n",
      "Epoch 192/200, Train Loss: 0.0009130477954107697, Validation Loss: 0.0008274476951180351\n",
      "\n",
      "Epoch 193/200, Train Loss: 0.0008942910384897802, Validation Loss: 0.0008236546899073772\n",
      "\n",
      "Epoch 194/200, Train Loss: 0.000877477785953826, Validation Loss: 0.000811352317834953\n",
      "\n",
      "Epoch 195/200, Train Loss: 0.0008623050230695953, Validation Loss: 0.0007945635847540365\n",
      "\n",
      "Epoch 196/200, Train Loss: 0.0008450760096441177, Validation Loss: 0.0007898058439235366\n",
      "\n",
      "Epoch 197/200, Train Loss: 0.0008280556000932074, Validation Loss: 0.0007668070713176378\n",
      "\n",
      "Epoch 198/200, Train Loss: 0.0008108294364129736, Validation Loss: 0.0007648908889793332\n",
      "\n",
      "Epoch 199/200, Train Loss: 0.0007967332210928706, Validation Loss: 0.0007512060858841453\n",
      "\n",
      "Epoch 200/200, Train Loss: 0.0007817465367599226, Validation Loss: 0.0007422945659888524\n",
      "\n",
      "MAE: 0.11096207052469254, MSE: 0.016260221600532532\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn import Module, Sequential\n",
    "from torch.nn import Linear, ReLU, LeakyReLU\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "file_path_train=r\"C:\\Users\\david\\Desktop\\test\\Google_Stock_Price\\Google_Stock_Price_Train.csv\"\n",
    "file_path_test=r\"C:\\Users\\david\\Desktop\\test\\Google_Stock_Price\\Google_Stock_Price_Test.csv\"\n",
    "\n",
    "df_train = pd.read_csv(file_path_train)\n",
    "df_test = pd.read_csv(file_path_test)\n",
    "\n",
    "df_train['Date'] = pd.to_datetime(df_train['Date'], format='%m/%d/%Y')\n",
    "df_test['Date'] = pd.to_datetime(df_test['Date'], format='%m/%d/%Y')\n",
    "df_train['Volume'] = df_train['Volume'].str.replace(',', '').astype(int)\n",
    "df_test['Volume'] = df_test['Volume'].str.replace(',', '').astype(int)\n",
    "\n",
    "X_train = df_train[['Open', 'High', 'Low', 'Close', 'Volume']].values\n",
    "X_test = df_test[['Open', 'High', 'Low', 'Close', 'Volume']].values\n",
    "y_train = df_train['Close'].values.reshape(-1, 1)\n",
    "y_test = df_test['Close'].values.reshape(-1, 1) \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "y_train_scaled = scaler.fit_transform(y_train)\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "y_test_scaled = scaler.fit_transform(y_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_scaled, dtype=torch.float32)\n",
    "\n",
    "class Neural_Network(nn.Module):\n",
    "    def __init__(self, num_features: int, num_classes: int):\n",
    "        super(Neural_Network, self).__init__()\n",
    "\n",
    "        self.layer_1_neurons = 50\n",
    "        self.layer_2_neurons = 20\n",
    "        self.layer_3_neurons = 10\n",
    "        self.output_neurons = 6\n",
    "\n",
    "        self.fc_input = nn.Sequential(\n",
    "            nn.Linear(num_features, self.layer_1_neurons),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.fc_l1 = nn.Sequential(\n",
    "            nn.Linear(self.layer_1_neurons, self.layer_2_neurons),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.fc_l2 = nn.Sequential(\n",
    "            nn.Linear(self.layer_2_neurons, self.layer_3_neurons),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.fc_output = nn.Sequential(\n",
    "            nn.Linear(self.layer_3_neurons, num_classes),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Linear regression layer\n",
    "        self.linear_regression = nn.Linear(self.output_neurons + num_features, 1)\n",
    "\n",
    "        self.optimizer = Adam(self.parameters(), lr=0.00005)\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_input(x)\n",
    "        x = self.fc_l1(x)\n",
    "        x = self.fc_l2(x)\n",
    "        output_nn = self.fc_output(x)\n",
    "        output_concat = torch.cat((output_nn, x), dim=1)  # Concatenate output with original features\n",
    "        output_linear = self.linear_regression(output_concat)\n",
    "        return output_linear\n",
    "\n",
    "    def train_model(self, train_loader, val_loader, num_epochs=200):\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            epoch_train_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.forward(inputs)\n",
    "                loss = self.loss_fn(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_train_loss += loss.item() * inputs.size(0)\n",
    "            epoch_train_loss /= len(train_loader.dataset)\n",
    "            train_losses.append(epoch_train_loss)\n",
    "\n",
    "            self.eval()\n",
    "            with torch.no_grad():\n",
    "                epoch_val_loss = 0.0\n",
    "                for inputs, labels in val_loader:\n",
    "                    outputs = self.forward(inputs)\n",
    "                    loss = self.loss_fn(outputs, labels)\n",
    "                    epoch_val_loss += loss.item() * inputs.size(0)\n",
    "                epoch_val_loss /= len(val_loader.dataset)\n",
    "                val_losses.append(epoch_val_loss)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss}, Validation Loss: {epoch_val_loss}\\n\")\n",
    "\n",
    "    def evaluate_model(self, X_test, y_test):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = self.forward(X_test)\n",
    "            mae = mean_absolute_error(y_test.detach().numpy(), predictions.detach().numpy())\n",
    "            mse = mean_squared_error(y_test.detach().numpy(), predictions.detach().numpy())\n",
    "            print(f\"MAE: {mae}, MSE: {mse}\")\n",
    "            return predictions\n",
    "        \n",
    "X_train_tensor, X_val_tensor, y_train_tensor, y_val_tensor = train_test_split(X_train_tensor, y_train_tensor, test_size=0.2)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "num_features = X_train_tensor.shape[1]\n",
    "num_classes = 1\n",
    "model = Neural_Network(num_features, num_classes)\n",
    "\n",
    "model.train_model(train_loader, val_loader)\n",
    "\n",
    "predictions = model.evaluate_model(X_test_tensor, y_test_tensor)\n",
    "\n",
    "predictions_denormalized = scaler.inverse_transform(predictions.detach().numpy())\n",
    "y_test_denormalized = scaler.inverse_transform(y_test_tensor.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
