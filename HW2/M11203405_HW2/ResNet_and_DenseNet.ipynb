{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ResNet"
      ],
      "metadata": {
        "id": "p8g_G_M_PyXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=1000):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self.make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self.make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self.make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self.make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "    def make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Example usage:\n",
        "resnet = ResNet(ResidualBlock, [2, 2, 2, 2])\n",
        "print(resnet)"
      ],
      "metadata": {
        "id": "SO1ajGkEPx5x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba705264-1d79-49a6-f6cd-0f1990f1c19c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)             (None, 112, 112, 64)         9472      ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " batch_normalization (Batch  (None, 112, 112, 64)         256       ['conv2d[0][0]']              \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " activation (Activation)     (None, 112, 112, 64)         0         ['batch_normalization[0][0]'] \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2  (None, 56, 56, 64)           0         ['activation[0][0]']          \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)           (None, 56, 56, 64)           36928     ['max_pooling2d[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_1 (Bat  (None, 56, 56, 64)           256       ['conv2d_1[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_1 (Activation)   (None, 56, 56, 64)           0         ['batch_normalization_1[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)           (None, 56, 56, 64)           36928     ['activation_1[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)           (None, 56, 56, 64)           4160      ['max_pooling2d[0][0]']       \n",
            "                                                                                                  \n",
            " batch_normalization_2 (Bat  (None, 56, 56, 64)           256       ['conv2d_2[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_3 (Bat  (None, 56, 56, 64)           256       ['conv2d_3[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " add (Add)                   (None, 56, 56, 64)           0         ['batch_normalization_2[0][0]'\n",
            "                                                                    , 'batch_normalization_3[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_2 (Activation)   (None, 56, 56, 64)           0         ['add[0][0]']                 \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)           (None, 56, 56, 64)           36928     ['activation_2[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_4 (Bat  (None, 56, 56, 64)           256       ['conv2d_4[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_3 (Activation)   (None, 56, 56, 64)           0         ['batch_normalization_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)           (None, 56, 56, 64)           36928     ['activation_3[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)           (None, 56, 56, 64)           4160      ['activation_2[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_5 (Bat  (None, 56, 56, 64)           256       ['conv2d_5[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_6 (Bat  (None, 56, 56, 64)           256       ['conv2d_6[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " add_1 (Add)                 (None, 56, 56, 64)           0         ['batch_normalization_5[0][0]'\n",
            "                                                                    , 'batch_normalization_6[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_4 (Activation)   (None, 56, 56, 64)           0         ['add_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)           (None, 56, 56, 64)           36928     ['activation_4[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_7 (Bat  (None, 56, 56, 64)           256       ['conv2d_7[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " activation_5 (Activation)   (None, 56, 56, 64)           0         ['batch_normalization_7[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)           (None, 56, 56, 64)           36928     ['activation_5[0][0]']        \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)           (None, 56, 56, 64)           4160      ['activation_4[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_8 (Bat  (None, 56, 56, 64)           256       ['conv2d_8[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " batch_normalization_9 (Bat  (None, 56, 56, 64)           256       ['conv2d_9[0][0]']            \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " add_2 (Add)                 (None, 56, 56, 64)           0         ['batch_normalization_8[0][0]'\n",
            "                                                                    , 'batch_normalization_9[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " activation_6 (Activation)   (None, 56, 56, 64)           0         ['add_2[0][0]']               \n",
            "                                                                                                  \n",
            " flatten (Flatten)           (None, 200704)               0         ['activation_6[0][0]']        \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 1000)                 2007050   ['flatten[0][0]']             \n",
            "                                                          00                                      \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 200951080 (766.57 MB)\n",
            "Trainable params: 200949800 (766.56 MB)\n",
            "Non-trainable params: 1280 (5.00 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DenseNet"
      ],
      "metadata": {
        "id": "rMnBghXlP0iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate, num_layers):\n",
        "        super(DenseBlock, self).__init__()\n",
        "        self.layers = nn.ModuleList([self.add_block(in_channels + i * growth_rate, growth_rate) for i in range(num_layers)])\n",
        "\n",
        "    def add_block(self, in_channels, out_channels):\n",
        "        block = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        )\n",
        "        return block\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = [x]\n",
        "        for layer in self.layers:\n",
        "            features.append(layer(torch.cat(features, 1)))\n",
        "        return torch.cat(features, 1)\n",
        "\n",
        "class TransitionBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(TransitionBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.bn(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.pool(out)\n",
        "        return out\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, growth_rate=12, reduction=0.5, num_classes=1000):\n",
        "        super(DenseNet, self).__init__()\n",
        "        self.growth_rate = growth_rate\n",
        "        self.num_blocks = num_blocks\n",
        "        self.reduction = reduction\n",
        "        self.num_features = 2 * growth_rate\n",
        "        self.conv1 = nn.Conv2d(3, self.num_features, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.dense_blocks = nn.ModuleList([self.make_dense_block(block, self.num_features + i * growth_rate, num_blocks[i]) for i in range(len(num_blocks))])\n",
        "        self.transition_blocks = nn.ModuleList([self.make_transition_block(self.num_features + i * growth_rate, int(self.num_features * reduction)) for i in range(len(num_blocks) - 1)])\n",
        "        self.bn = nn.BatchNorm2d(self.num_features + sum(num_blocks) * growth_rate)\n",
        "        self.fc = nn.Linear(self.num_features + sum(num_blocks) * growth_rate, num_classes)\n",
        "\n",
        "    def make_dense_block(self, block, in_channels, num_blocks):\n",
        "        layers = []\n",
        "        for _ in range(num_blocks):\n",
        "            layers.append(block(in_channels, self.growth_rate, num_blocks))\n",
        "            in_channels += self.growth_rate\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def make_transition_block(self, in_channels, out_channels):\n",
        "        return TransitionBlock(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        for i in range(len(self.num_blocks)):\n",
        "            out = self.dense_blocks[i](out)\n",
        "            if i != len(self.num_blocks) - 1:\n",
        "                out = self.transition_blocks[i](out)\n",
        "        out = self.bn(out)\n",
        "        out = F.relu(out)\n",
        "        out = F.adaptive_avg_pool2d(out, (1, 1))\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Example usage:\n",
        "num_blocks = [6, 12, 24, 16]  # Number of dense blocks in each stage\n",
        "densenet = DenseNet(DenseBlock, num_blocks)\n",
        "print(densenet)"
      ],
      "metadata": {
        "id": "Xtsaa0NnEzyF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}